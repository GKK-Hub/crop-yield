# This file generated by Quarto; do not edit by hand.
# shiny_mode: core

from __future__ import annotations

from pathlib import Path
from shiny import App, Inputs, Outputs, Session, ui




def server(input: Inputs, output: Outputs, session: Session) -> None:
    import os
    os.environ["GIT_PYTHON_REFRESH"] = "quiet"

    # ========================================================================

    import pandas as pd
    import numpy as np
    import random
    import warnings

    warnings.filterwarnings('ignore')

    SEED = 42

    random.seed(SEED)
    np.random.seed(SEED)

    pd.set_option('display.float_format', '{:.2f}'.format)

    # ========================================================================

    import sys
    from pathlib import Path

    # Add src/ to Python path
    sys.path.append(str(Path.cwd().parent / "src"))

    # ========================================================================

    df = pd.read_csv('..\\data\\combined_file\\combined.csv')
    # df.head()

    # ========================================================================

    df.columns

    # ========================================================================

    pd.DataFrame(df.columns, columns=['Parameters / Columns'])

    # ========================================================================

    # df.info()

    # ========================================================================

    pd.DataFrame(df['dist_name'].unique(), columns=['Districts'])

    # ========================================================================

    # Renaming the column names for better readability
    df.rename(columns={
                       'rice_area_1000_ha': 'rice_area',
                       'rice_production_1000_tons': 'rice_production',
                       'rice_yield_kg_per_ha': 'rice_yield',
                       'avg_evapotranspiration_mm': 'avg_act_evapotranspiration',
                       'avg_pot_evapotranspiration_mm': 'avg_pot_evapotranspiration',
                       'rice_irrigated_area_1000_ha': 'rice_irrigated_area',
                       'avg_precipitation_mm': 'avg_precipitation',
                       'avg_water_deficit_mm': 'avg_water_deficit',
                       'nitrogen_kharif_consumption_tons': 'nitrogen_kharif',
                       'nitrogen_rabi_consumption_tons': 'nitrogen_rabi',
                       'phosphate_kharif_consumption_tons': 'phosphate_kharif',
                       'phosphate_rabi_consumption_tons': 'phosphate_rabi',
                       'potash_kharif_consumption_tons': 'potash_kharif',
                       'potash_rabi_consumption_tons': 'potash_rabi',
                       'total_kharif_consumption_tons': 'total_kharif',
                       'total_rabi_consumption_tons': 'total_rabi'
                       },
              inplace=True)

    # ========================================================================

    df.drop(columns=['nitrogen_kharif',
                     'nitrogen_rabi',
                     'phosphate_kharif',
                     'phosphate_rabi',
                     'potash_kharif',
                     'potash_rabi'],
            inplace=True)

    # ========================================================================

    # df.columns

    # ========================================================================

    import seaborn as sns
    import matplotlib.pyplot as plt

    # ========================================================================

    n_years = df['year'].nunique()
    min_year= df['year'].min()
    max_year = df['year'].max()

    print(f"The rice yield dataset has {n_years} years' data with minimum year: {min_year} and maximum year: {max_year}")

    # ========================================================================

    n_districts = df['dist_name'].nunique()

    print(f'There are {n_districts} unique districts in the dataset')

    # ========================================================================

    district_data_count = pd.DataFrame(df['dist_name'].value_counts())
    district_data_count

    # ========================================================================

    low_data_districts = district_data_count[district_data_count['count'] <= 20].index
    low_data_districts

    # ========================================================================

    district_mask = ~df['dist_name'].isin(low_data_districts)
    district_mask

    # ========================================================================

    filtered_df = df[district_mask]
    filtered_df['dist_name'].unique()

    # ========================================================================

    filtered_df.info()

    # ========================================================================

    filtered_df['year'].nunique()

    # ========================================================================

     # The `year_rescale` feature contains values between 1 and 26 which corresponds to 1990 and 2015.
    filtered_df['year_rescale'] = abs((filtered_df['year'].max() + 1) - filtered_df['year'])
    filtered_df['year_rescale'].min(), filtered_df['year_rescale'].max()

    # ========================================================================

    from shiny.express import input, render, ui
    from htmltools import TagList, tags

    # ========================================================================

    dist_variables = ['dist_name',
                 'actual_evapotranspiration',
                 'rice_area',
                 'rice_production',
                 'rice_yield',
                 'rice_irrigated_area',
                 'maximum_temperature',
                 'minimum_temperature', 
                 'precipitation',
                 'water_deficit',
                 'total_kharif',
                 'total_rabi'
                ]

    dist_remarks = {
        "dist_name": "Most of the districts have 26 years' data, except a few.",
        "actual_evapotranspiration": "Almost normally distributed with no outliers or skewness.",
        "rice_area": "The data is approximately normal with right skew.",
        "rice_production": "The data is approximately normal with slight right skew.",
        "rice_yield": 'The `rice_yield` variable is approximately normal with left skew.',
        "rice_irrigated_area": "The data is approximately normal with right skew.",
        "maximum_temperature": "Distribution is bi-modal",
        "minimum_temperature": "Distribution is bi-modal.",
        "precipitation": "The data is approximately normal with slight right skew.",      
        "water_deficit": "Almost normally distributed with no outliers or skewness.",
        "total_kharif": "Almost normally distributed",
        "total_rabi": "Almost normally distributed"
    }

    # ========================================================================

    # UI elements
    ui.input_select("dist_var", "Choose Variable", choices=dist_variables)

    # Plot output
    @render.plot(width=900, height=400)
    def dist_plot():
        col = input.dist_var()

        if col != 'dist_name':
          fig, ax = plt.subplots(1, 2)

          plt.style.use('fivethirtyeight')
          # Axes 1: original distribution
          sns.distplot(filtered_df[col], ax=ax[0]);
          ax[0].set_xlabel(col, fontsize=11)
          ax[0].set_ylabel("Density", fontsize=11)
      

          # Axes 2: box plot of the original distribution
          sns.boxplot(filtered_df[col], ax=ax[1]);
          ax[1].set_ylabel(col, fontsize=11)
    
        else:
          plt.style.use('fivethirtyeight')
          fig, ax = plt.subplots(1)
          filtered_df[col].value_counts().plot(kind='bar');

          ax.set_ylim(0, 27)
          ax.set_yticks(range(1, 27, 5))
          ax.set_xlabel('') # Hide the x-axis label 
          ax.set_xticklabels(ax.get_xticklabels(), fontsize=11)
          ax.set_yticklabels(ax.get_yticklabels(), fontsize=11)
          ax.xaxis.grid(False)

    # Remarks output
    @render.ui
    def dist_remarks_ui():
        col = input.dist_var()
        return TagList(tags.p("Remarks for ", tags.code(col), ": ", tags.strong(dist_remarks.get(col))))

    # ========================================================================

    corr_variables = ['actual_evapotranspiration',
                 'rice_area',
                 'rice_production',
                 'rice_irrigated_area',
                 'maximum_temperature',
                 'minimum_temperature', 
                 'precipitation',
                 'water_deficit',
                 'total_kharif',
                 'total_rabi'
                ]

    corr_remarks = {
        "actual_evapotranspiration": "No Correlation",
        "rice_area": "No Correlation",
        "rice_production": "No Correlation.",
        "rice_irrigated_area": "No Correlation.",
        "maximum_temperature": "No Correlation",
        "minimum_temperature": "No Correlation.",
        "precipitation": "No Correlation.",      
        "water_deficit": "No Correlation.",
        "total_kharif": "No Correlation",
        "total_rabi": "No Correlation"
    }

    # ========================================================================

    # UI elements
    ui.input_select("corr_var", "Choose Variable", choices=corr_variables)

    @render.plot(width=900, height=400)
    def corr_plot():
        col = input.corr_var()

        corr = filtered_df['rice_yield'].corr(filtered_df[col])
        # print(f'Correlation: {corr:.2f}')

        # 2. Scatter plot with regression line
        plt.style.use('fivethirtyeight')

        fig, ax = plt.subplots(1)
        sns.regplot(x=col, y='rice_yield', data=filtered_df, ax=ax)
        plt.title(f'Scatter Plot (Correlation: {corr:.2f})', fontdict={'fontsize': 12, 'fontweight': 'bold', 'family': 'Arial'})
        ax.set_xlabel(ax.get_xlabel(), fontsize=11)
        ax.set_ylabel(ax.get_ylabel(), fontsize=11)
        ax.set_xticklabels(ax.get_xticklabels(), fontsize=10)
        ax.set_yticklabels(ax.get_yticklabels(), fontsize=10)

    @render.ui
    def corr_remarks_ui():
        col = input.corr_var()
        return TagList(tags.p("Remarks for ", tags.code(col), ": ", tags.strong(corr_remarks.get(col))))

    # ========================================================================

    corr_df = filtered_df.drop(columns=['dist_name', 'year', 'year_rescale']).corr()

    # ========================================================================

    import numpy as np
    import matplotlib.colors as mcolors

    @render.plot(width=900, height=700)
    def cor_plot():

        plt.style.use('fivethirtyeight')

        fig, ax = plt.subplots(1)
        # sky_blue_cmap = sns.light_palette('deepskyblue', as_cmap=True, reverse=True)
        cmap = mcolors.LinearSegmentedColormap.from_list('custom_deepskyblue', ['deepskyblue', 'white', 'deepskyblue'], N=100)
        norm = mcolors.CenteredNorm()
        # Plot the heatmap
        sns.heatmap(corr_df,
                    annot=True,
                    cmap=cmap,
                    vmin=-1,
                    vmax=1,
                    fmt='.1f',
                    mask=np.triu(np.ones_like(corr_df, dtype=bool)),
                    norm=norm
                    )
        plt.title('Heatmap of Correlation Coefficient Matrix', fontdict={'fontsize': 12, 'fontweight': 'bold', 'family': 'Arial'});
        ax.set_xticklabels(ax.get_xticklabels(), fontsize=10);
        ax.set_yticklabels(ax.get_yticklabels(), fontsize=10);
        ax.xaxis.grid(False)
        ax.yaxis.grid(False)

    # ========================================================================

    filtered_df = filtered_df.drop(columns=['year_rescale'])

    # ========================================================================

    df = filtered_df.sort_values(['year', 'dist_name'])
    df

    # Split
    train_df = df[df['year'] <= 2010].copy()
    test_df = df[df['year'] > 2010].copy()

    # ========================================================================

    train_df.head(30)

    # ========================================================================

    train_df.info()

    # ========================================================================

    test_df.info()

    # ========================================================================

    df['split'] = df['year'].apply(lambda x: "Train" if x <= 2010 else "Test")
    df.tail()

    # ========================================================================

    plot_columns = [
        "maximum_temperature",
        "minimum_temperature",
        "precipitation",
        "actual_evapotranspiration",
        "rice_yield",
        "rice_production",
        "water_deficit"
    ]

    # ========================================================================

    ui.input_select('feature', 'Choose a feature', choices=plot_columns)

    @render.plot(width=900, height=400)
    def train_test_plot():
      plt.style.use('fivethirtyeight')

      fig, ax = plt.subplots(1)
      sns.scatterplot(data=df,
                      x='year',
                      y=input.feature(),
                      hue='split',
                      ax=ax)
      plt.title('Train test split', fontdict={'fontsize': 13, 'fontweight': 'bold', 'family': 'Arial'})
      plt.xlabel('Year')
      plt.legend(
            bbox_to_anchor=(1, 1),  # x=1.05 (right outside), y=1 (top aligned)
            loc="upper left",
            ncol=2,
            fontsize=12)
      ax.set_xlabel(ax.get_xlabel(), fontsize=11)
      ax.set_ylabel(ax.get_ylabel(), fontsize=11)
      ax.set_xticklabels(ax.get_xticklabels(), fontsize=10);
      ax.set_yticklabels(ax.get_yticklabels(), fontsize=10);

    # ========================================================================

    X_train = train_df.drop(columns=['rice_yield'])
    y_train = train_df['rice_yield']

    X_test = test_df.drop(columns=['rice_yield'])
    y_test = test_df['rice_yield']

    # ========================================================================

    from sklearn import set_config
    from sklearn.model_selection import TimeSeriesSplit

    set_config(display='text')

    N = X_train.shape[0]
    MIN_TEST_SIZE = 30
    n_splits = N // MIN_TEST_SIZE - 1 # To ensure we get the atleast the same size for every split

    tscv = TimeSeriesSplit(n_splits=n_splits)
    tscv

    # ========================================================================

    # for i, (train_idx, test_idx) in enumerate(tscv.split(X_train)):
    #     print(f"Split {i+1}: Train size = {len(train_idx)}, Test size = {len(test_idx)}")

    # ========================================================================

    # Column Selector
    from sklearn.compose import make_column_selector as selector

    numeric_columns_selector = selector(dtype_exclude=object)
    categoric_columns_selector = selector(dtype_include=object)

    # ========================================================================

    numeric_columns = numeric_columns_selector(X_train)
    categoric_columns = categoric_columns_selector(X_train)

    # ========================================================================

    from sklearn.compose import ColumnTransformer
    from sklearn.preprocessing import OneHotEncoder, MinMaxScaler

    categoric_transformer = OneHotEncoder(handle_unknown='ignore')
    numeric_transformer = MinMaxScaler()

    preprocessor = ColumnTransformer(transformers=[('categoric', categoric_transformer, categoric_columns),
                                                   ('numeric', numeric_transformer, numeric_columns)],
                                    remainder='passthrough',
                                    verbose_feature_names_out=False
                                    )

    # ========================================================================

    import numpy as np
    import pandas as pd
    import mlflow
    import mlflow.sklearn
    from sklearn.dummy import DummyRegressor
    from sklearn.metrics import mean_squared_error, r2_score
    from mlflow.models.signature import infer_signature
    from utils.plots import prediction_error_display
    from utils.plots import validation_curve_display

    # Ensure X_train and X_test are DataFrames or convertible to one
    X_train_df = pd.DataFrame(X_train)
    X_test_df = pd.DataFrame(X_test)
    y_train_series = pd.Series(y_train)
    y_test_series = pd.Series(y_test)

    # Set tracking URI and experiment name
    mlflow.set_tracking_uri("file:C:/Users/DELL/Desktop/crop_yield/eda/mlruns")
    mlflow.set_experiment("dummy_baseline")

    # Start MLflow run
    with mlflow.start_run(run_name="Dummy Regressor"):

        # Train the Dummy Regressor
        dummy = DummyRegressor(strategy="mean")
        dummy.fit(X_train_df, y_train_series)

        # Predict
        y_pred = dummy.predict(X_test_df)

        # Compute metrics
        mse = mean_squared_error(y_test_series, y_pred)
        r2 = r2_score(y_test_series, y_pred)

        # Log parameters and metrics
        mlflow.log_param("strategy", "mean")
        mlflow.log_metric("mse", mse)
        mlflow.log_metric("r2", r2)

        # Log the model with signature
        signature = infer_signature(X_test_df, y_pred)
        mlflow.sklearn.log_model(
            sk_model=dummy,
            name="dummy_model",  # Use `name=` instead of `artifact_path=`
            signature=signature,
            registered_model_name='baseline_dummy',
            input_example=X_test_df.head(5)
        )

        # Create and save the prediction error plot
        # fig, ax = plt.subplots(figsize=(15, 6))
        display = prediction_error_display(y_test=y_test, y_pred=y_pred)
        # display.plot(ax=ax)
        # plt.title("Prediction Error Plot")

        # Save and log as artifact
        artifact_dir = "mlflow_artifacts"
        os.makedirs(artifact_dir, exist_ok=True)
        plot_path = os.path.join(artifact_dir, "prediction_error.png")
        display.savefig(plot_path)
        plt.close()

        mlflow.log_artifact(plot_path)

        mlflow.end_run()

    # print(mlflow.search_experiments()) 

    # ========================================================================

    from sklearn.svm import SVR
    from sklearn.pipeline import Pipeline
    from sklearn.model_selection import GridSearchCV

    mlflow.set_tracking_uri("file:C:/Users/DELL/Desktop/crop_yield/eda/mlruns")
    mlflow.set_experiment("svr_reg")

    artifact_dir = "mlflow_artifacts_svr"
    os.makedirs(artifact_dir, exist_ok=True)

    # Start MLflow run
    with mlflow.start_run(run_name="SVR Regressor"):

      param_grid = {
          'svr__C': [800, 1000, 1100],   # narrowed down from plot results
          'svr__epsilon': [2, 2.5, 3],    # narrowed down from plot results
          'svr__gamma': [0.00001, 0.0001, 0.001],  # narrowed down
          'svr__kernel': ['rbf', 'poly', 'linear']    # chose the best-performing kernels from plots
      }

      # Set up the pipeline with GridSearchCV
      svr_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('svr', SVR(max_iter=500))])
      svr_model = GridSearchCV(svr_pipeline, param_grid, cv=tscv, scoring='neg_root_mean_squared_error', n_jobs=1)

      # Fit GridSearchCV
      svr_model.fit(X_train, y_train)

      best_model = svr_model.best_estimator_
      y_pred_test = best_model.predict(X_test)
      test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))

      # CV results
      cv_results = pd.DataFrame(svr_model.cv_results_).sort_values(by='rank_test_score').head(5)
      cv_results = cv_results[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']]
      cv_results['mean_test_score'] = -cv_results['mean_test_score']
      cv_results.to_csv("mlflow_artifacts_svr/svr_cv_results.csv", index=False)

      #MLflow logging
      mlflow.log_params(svr_model.best_params_)
      mlflow.log_metric("train_rmse", np.sqrt(-svr_model.best_score_))
      mlflow.log_metric("test_rmse", test_rmse)
      # mlflow.sklearn.log_model(best_model, name="model")
      mlflow.sklearn.log_model(
        best_model,
        name="model",
        input_example=X_test.iloc[:1]
    )

      mlflow.log_artifact("mlflow_artifacts_svr/svr_cv_results.csv")

      # fig, ax = plt.subplots()
      display = prediction_error_display(y_test=y_test, y_pred=y_pred_test)

      plot_path = os.path.join(artifact_dir, "acutual_vs_predicted.png")
      display.savefig(plot_path)
      # plt.close()

      mlflow.log_artifact(plot_path)

      mlflow.end_run()

    # ========================================================================

    exp = mlflow.set_experiment("svr_reg")
    print("Experiment ID:", exp.experiment_id)
    print("Tracking URI:", mlflow.get_tracking_uri())

    # ========================================================================

    import matplotlib.pyplot as plt
    from sklearn.model_selection import ValidationCurveDisplay, validation_curve
    from sklearn.model_selection import TimeSeriesSplit
    import os
    import mlflow

    # Define your parameter range
    # param_range = [100, 300, 500, 700, 1000, 1100, 1200]
    # tscv = TimeSeriesSplit(n_splits=5)

    # Start MLflow run
    with mlflow.start_run(run_name="Validation Curve - SVR__C"):
      fig, ax = plt.subplots(figsize=(14, 8))
      param_range = [0, 100, 500, 800, 1000]


      # Create and save validation curve
      ValidationCurveDisplay.from_estimator(
          estimator=svr_pipeline,
          X=X_train,
          y=y_train,
          param_name="svr__C",  # use correct param name in pipeline
          param_range=param_range,
          cv=tscv,
          scoring="neg_root_mean_squared_error",
          n_jobs=1,
          negate_score=True,
          ax=ax
      )

      # Set title and save the current plot
      plt.title("Validation Curve for SVR (C)")
  
      artifact_dir = "mlflow_artifacts_svr"
      os.makedirs(artifact_dir, exist_ok=True)
      plot_path = os.path.join(artifact_dir, "validation_curve_svr_C.png")
      plt.savefig(plot_path)
      plt.close()

        # Log the plot to MLflow
      mlflow.log_artifact(plot_path)

    # ========================================================================

    from sklearn.svm import SVR
    from sklearn.tree import DecisionTreeRegressor
    from sklearn.ensemble import ExtraTreesRegressor, AdaBoostRegressor
    from sklearn.pipeline import Pipeline
    from xgboost import XGBRegressor

    candidate_models = {
        "Support Vector Regressor": {
            "pipeline": Pipeline(steps=[('preprocessor', preprocessor), ('svr', SVR(max_iter=500))]),
            "param_grid": {
                "svr__C": [800, 1000, 1100],
                "svr__epsilon": [2, 2.5, 3],
                "svr__gamma": [0.00001, 0.0001, 0.001],
            }
        },
        "Extra Trees Regressor": {
            "pipeline": Pipeline(steps=[('preprocessor', preprocessor), ('etr', ExtraTreesRegressor())]),
            "param_grid": {
                "etr__min_samples_split": [2, 3, 5, 10, 15, 20],
                "etr__n_estimators": [2, 3, 5, 10, 50, 75],
                "etr__max_depth": [2, 3, 4, 5, 6, 7, 8, 9, 10],
                "etr__min_samples_leaf": [2, 3, 4, 5, 6, 7, 8, 9, 10]
            }
        },
        "Ada Boost Regressor": {
            "pipeline": Pipeline(steps=[('preprocessor', preprocessor), 
                                        ('ada', AdaBoostRegressor(DecisionTreeRegressor()))]),
            "param_grid": {
                "ada__n_estimators": [5, 10, 20, 50, 100, 125],
                "ada__learning_rate": [0.001, 0.01, 0.02, 0.04, 0.06],
            }
        },
        "XGBoost Regressor": {
            "pipeline": Pipeline(steps=[('preprocessor', preprocessor),
                                        ('xgb', XGBRegressor())]),
            "param_grid": {
                "xgb__n_estimators": [5, 10, 20, 50, 100, 125],
                "xgb__learning_rate": [0.01, 0.1, 0.2],
                "xgb__max_depth": [2, 3, 5, 7]
            }
        }
    }

    # ========================================================================

    import os

    for model_name, model_data in candidate_models.items():
        pipe = model_data["pipeline"]
        param_grid = model_data["param_grid"]

        mlflow.set_experiment(f"Validation_{model_name.replace(' ', '_')}")

        with mlflow.start_run(run_name="validation_curves"):
            folder_path = f"validation_curves/{model_name.replace(' ', '_')}"
            os.makedirs(folder_path, exist_ok=True)

            for param, values in param_grid.items():
                fig = validation_curve_display(
                    estimator=pipe,
                    X=X_train,
                    y=y_train,
                    param_name=param,
                    param_range=values,
                    cv=tscv,
                    scoring='neg_root_mean_squared_error',
                    negate_score=True,
                );

                fig_path = os.path.join(folder_path, f"{param.split('__')[1]}.png")
                fig.savefig(fig_path)
                mlflow.log_artifact(fig_path)

    # ========================================================================

    # from shiny import render, ui, reactive
    # from pathlib import Path
    # from PIL import Image
    # import matplotlib.pyplot as plt

    # model_options = list(candidate_models.keys())

    # @reactive.Calc
    # def selected_model_folder():
    #     return Path(f"validation_curves/{"_".join(input.model().split())}")

    # @render.ui
    # def dropdown():
    #     return ui.input_select("model", "Select a model", choices=model_options)

    # @render.plot
    # def show_all_param_plots():
    #     folder = selected_model_folder()
    #     params = [f for f in folder.glob("*.png")]

    #     # fig, axs = plt.subplots(len(params), 1, figsize=(8, 20))
    #     fig, axs = plt.subplots(len(params), 1, figsize=(10, 10 * len(params)))  # Increase height per plot

    #     if len(params) == 1:
    #         axs = [axs]

    #     for ax, param_file in zip(axs, params):
    #         img = Image.open(param_file)
    #         ax.imshow(img)
    #         ax.set_title(param_file.stem)
    #         ax.axis("off")

    #     # plt.tight_layout()
    #     return fig

    # ========================================================================

    from shiny import reactive, render, ui, req
    from pathlib import Path
    from PIL import Image
    import matplotlib.pyplot as plt

    # Base path to saved validation curve plots
    base_path = Path("validation_curves")

    # Automatically extract model names from subfolders
    model_names = sorted([f.name for f in base_path.iterdir() if f.is_dir()])
    default_model = model_names[0] if model_names else None

    # Model dropdown
    @render.ui
    def dropdown_model():
        return ui.input_select(
            id="model",
            label="Select Model",
            choices=model_names,
            selected=default_model,
        )

    # Dynamically get folder path of selected model
    @reactive.Calc
    def model_folder():
        return base_path / input.model()

    # Dynamically list available parameter plots in that model's folder
    @reactive.Calc
    def available_params():
        folder = model_folder()
        return sorted([f.stem for f in folder.glob("*.png")])

    # Parameter dropdown, reacts to model change
    @render.ui
    def dropdown_param():
        params = available_params()
        default_param = params[0] if params else None
        return ui.input_select(
            id="param",
            label="Select Parameter",
            choices=params,
            selected=default_param,
        )

    # Show only one selected plot
    @render.plot
    def show_plot():
        # req(input.param())
        # req(input.model())
        folder = model_folder()
        param_file = folder / f"{input.param()}.png"

        # if not param_file.exists():
        #     raise FileNotFoundError(f"Missing: {param_file}")

        if not param_file.exists():
            return None  # silently skip rendering

        img = Image.open(param_file)
        fig, ax = plt.subplots(figsize=(10, 20))
        ax.imshow(img)
        ax.set_title(f"{input.model()} — {input.param()}")
        ax.axis("off")
        return fig

    # ========================================================================

    from sklearn.model_selection import GridSearchCV
    from sklearn.metrics import mean_squared_error

    candidate_models = {
        "svr": {
            "estimator": Pipeline(steps=[('preprocessor', preprocessor), ('svr', SVR(max_iter=500))]),
            "param_grid": {
                "svr__C": [800, 1000, 1100],
                "svr__epsilon": [2, 2.5, 3],
                "svr__gamma": [0.00001, 0.0001, 0.001],
            }
        },
        "extratrees": {
            "estimator": Pipeline(steps=[('preprocessor', preprocessor), ('etr', ExtraTreesRegressor())]),
            "param_grid": {
                "etr__max_depth": [5, 10, 15],
                "etr__n_estimators": [50, 100],
                "etr__min_samples_split": [2, 5],
                "etr__min_samples_leaf": [1, 2]
            }
        },
        "xgboost": {
            "estimator": Pipeline(steps=[('preprocessor', preprocessor),
                                        ('xgb', XGBRegressor())]),
            "param_grid": {
                "xgb__n_estimators": [50, 100],
                "xgb__max_depth": [3, 5],
                "xgb__learning_rate": [0.05, 0.1]
            }
        },
        "adaboost": {
            "estimator": Pipeline(steps=[('preprocessor', preprocessor), 
                                        ('ada', AdaBoostRegressor(DecisionTreeRegressor()))]),
            "param_grid": {
                "ada__n_estimators": [50, 100],
                "ada__learning_rate": [0.05, 0.1, 0.5]
            }
        }
    }

    cv_results_dict = {}

    mlflow.set_tracking_uri("file:C:/Users/DELL/Desktop/crop_yield/eda/mlruns")
    mlflow.set_experiment("Hyperparameter_Optimization")

    for model_name, model_info in candidate_models.items():
        with mlflow.start_run(run_name=f"{model_name}_gridsearch"):
            estimator = model_info["estimator"]
            param_grid = model_info["param_grid"]

            grid = GridSearchCV(
                estimator=estimator,
                param_grid=param_grid,
                scoring="neg_root_mean_squared_error",
                cv=tscv,
            )

            grid.fit(X_train, y_train)

            # Get best model & test set RMSE
            best_model = grid.best_estimator_
            y_pred = best_model.predict(X_test)
            test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))

            # Log best parameters and test score
            mlflow.log_params(grid.best_params_)
            mlflow.log_metric("train_rmse", -grid.best_score_)
            mlflow.log_metric("test_rmse", test_rmse)

            # Save and log cv_results
            top5 = pd.DataFrame(grid.cv_results_).sort_values("rank_test_score").head(5)
            full_df = pd.DataFrame(grid.cv_results_)

            os.makedirs(f"grid_search/{model_name}", exist_ok=True)
            top5.to_csv(f"grid_search/{model_name}/top5.csv", index=False)
            full_df.to_csv(f"grid_search/{model_name}/full.csv", index=False)

            mlflow.log_artifact(f"grid_search/{model_name}/top5.csv")
            mlflow.log_artifact(f"grid_search/{model_name}/full.csv")

            # Store top5 for Shiny table view
            cv_results_dict[model_name] = top5

    # ========================================================================

    # *** Next: Try with Multi objective hyperparameter optimization in Optuna ***

    import optuna
    from optuna_integration.mlflow import MLflowCallback
    from sklearn.model_selection import cross_val_score

    # Mlflow integration
    mlflc = MLflowCallback("file:./mlruns", metric_name='rmse')

    def objective(trial):
        # Suggest hyperparameters
        # C = trial.suggest_float('C', 1e0, 1e3, log=True)
        # epsilon = trial.suggest_float('epsilon', 1e-3, 1.0, log=True)
        # gamma = trial.suggest_float('gamma', 1e-4, 1e-1, log=True)
        # kernel = trial.suggest_categorical('kernel', ['rbf', 'poly', 'sigmoid'])

        # # Define model
        # svr = SVR(C=C, epsilon=epsilon, gamma=gamma, kernel=kernel)

        # # Build pipeline (reuse your preprocessing steps)
        # model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
        #                                  ('svr', svr)])

        # model_pipeline.fit(X_train, y_train)

        # y_pred = model_pipeline.predict(X_test)
        # rmse = np.sqrt(mean_squared_error(y_test, y_pred))

        # with mlflow.start_run(nested=True):
        #     mlflow.log_params({
        #         'C': C, 
        #         'epsilon': epsilon,
        #         'gamma': gamma,
        #         'kernel': kernel})
        #     mlflow.log_metric('rmse', rmse)
        # return rmse

        params = {
            "C": trial.suggest_float("C", 0.1, 100, log=True),
            "epsilon": trial.suggest_float("epsilon", 0.01, 1.0, log=True),
            "gamma": trial.suggest_float('gamma', 1e-4, 1e-1, log=True),
            "kernel": trial.suggest_categorical("kernel", ["rbf", "poly", "linear"])
        }
    
        model = Pipeline([
            ('preprocessor', preprocessor),
            ('svr', SVR(**params))
        ])

        # rmse
        rmse_scores = cross_val_score(model, X_train, y_train, cv=tscv, scoring='neg_root_mean_squared_error')
        rmse = -np.mean(rmse_scores)

        # mae
        mae_scores = cross_val_score(model, X_train, y_train,cv=tscv, scoring='neg_mean_absolute_error' )
        mae = -np.mean(mae_scores)

        return rmse, mae

    # ========================================================================

    study = optuna.create_study(directions=['minimize', 'minimize'],
                                study_name='SVR Optimization', 
                                storage="sqlite:///./optuna_svr.sqlite3",  
                                load_if_exists=True)
    study.optimize(objective, n_trials=30, callbacks=[mlflc])

    # ========================================================================

    from sklearn.metrics import mean_absolute_error
    # best_params = study.best_trial.params
    best_params = min(study.best_trials, key=lambda t: t.values[0]).params # Choosing `rmse` to choose best parameters
    # print("Selected trial params:", best_by_rmse.params)


    # --------------------------------
    # 4. Train Best Model from Optuna
    # --------------------------------
    best_model = Pipeline(steps=[('preprocessor', preprocessor), ('svr', SVR(**best_params))])
    best_model.fit(X_train, y_train)

    # --------------------------------
    # 5. Log & Register Best Model in MLflow
    # --------------------------------
    mlflow.set_experiment("Optuna Best SVR")

    with mlflow.start_run(run_name="SVR_Optuna_Best"):
        mlflow.log_params(best_params)
        preds = best_model.predict(X_train)
        rmse = np.sqrt(mean_squared_error(y_train, preds))
        mae = mean_absolute_error(y_train, preds)

        mlflow.log_metric("rmse", rmse)
        mlflow.log_metric("mae", mae)

        # Log model
        mlflow.sklearn.log_model(
            artifact_path="model",
            sk_model=best_model,
            registered_model_name="Best_SVR"  # model registry name
        )
    print("Best model registered in MLflow as 'RiceYield_SVR'")

    # ========================================================================

    dataset_path = "C:/Users/DELL/Desktop/crop_yield/data/combined_file/combined.csv" 
    # Alternate: ..\\data\\combined_file\\combined.cs
    df = pd.read_csv(dataset_path)

    # Create dataset registration experiment
    mlflow.set_experiment("Dataset_Registration")

    with mlflow.start_run(run_name="Dataset_v1") as dataset_run:
        mlflow.log_artifact(dataset_path, artifact_path="dataset")
        mlflow.set_tag("dataset_version", "v1")
        mlflow.set_tag("description", "Cleaned dataset used for all model training")
        dataset_run_id = dataset_run.info.run_id

    # ========================================================================

    # def objective_svr(trial):
    #     params = {
    #         "C": trial.suggest_float("C", 0.1, 100, log=True),
    #         "epsilon": trial.suggest_float("epsilon", 0.01, 1.0, log=True),
    #         "gamma": trial.suggest_float('gamma', 1e-4, 1e-1, log=True),
    #         "kernel": trial.suggest_categorical("kernel", ["rbf", "poly", "linear"])
    #     }
    
    #     model = Pipeline([
    #         ('preprocessor', preprocessor),
    #         ('svr', SVR(**params))
    #     ])

    #     scores = cross_val_score(model, X_train, y_train, cv=tscv, scoring='neg_root_mean_squared_error')
    #     return -np.mean(scores)
    
    # def objective_etr(trial):
    #     params = {"n_estimators": trial.suggest_int("n_estimators", 100, 1000),
    #               "max_features": trial.suggest_float("max_features", 0.1, 1.0),
    #               "min_samples_split": trial.suggest_float("min_samples_split", 0.1, 1.0),
    #               "min_samples_leaf": trial.suggest_float("min_samples_leaf", 0.01, 0.5)
    #               }

    #     model = Pipeline([('preprocessor', preprocessor),
    #                       ('etr', ExtraTreesRegressor(**params))])

    #     scores = cross_val_score(model, X_train, y_train, cv=tscv, scoring='neg_root_mean_squared_error')
    #     return -np.mean(scores)

    # def objective_xgb(trial):
    #     params = {
    #         "max_depth": trial.suggest_int("max_depth", 3, 15),
    #         "n_estimators": trial.suggest_int("n_estimators", 100, 1000),
    #         "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
    #         "subsample": trial.suggest_float("subsample", 0.5, 1.0),
    #         "colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0),
    #         "gamma": trial.suggest_float("gamma", 0.0, 5.0)
    #     }

    #     model = Pipeline([('preprocessor', preprocessor),
    #                       ('xgb', XGBRegressor(**params))])

    #     scores = cross_val_score(model, X_train, y_train, cv=tscv, scoring='neg_root_mean_squared_error')
    #     return -np.mean(scores)

    # def objective_ada(trial):
    #     params = {
    #         "n_estimators": trial.suggest_int("n_estimators", 50, 500),
    #         "learning_rate": trial.suggest_float("learning_rate", 0.01, 2.0),
    #         "loss": trial.suggest_categorical("loss", ["linear", "square", "exponential"])
    #     }

    #     model = Pipeline([('preprocessor', preprocessor),
    #                       ('ada', AdaBoostRegressor(**params))])

    #     scores = cross_val_score(model, X_train, y_train, cv=tscv, scoring='neg_root_mean_squared_error')
    #     return -np.mean(scores)

    # ========================================================================

    # models_objectives = {
    #     "svr": objective_svr, # We want to specify how to set params in `sklearn`
    #     "etr": objective_etr,
    #     "xgb": objective_xgb,
    #     "ada": objective_ada
    # }

    # os.makedirs("optuna_studies", exist_ok=True)

    # # Loop through each model and optimize
    # for model_name, objective  in models_objectives.items():
    #     print(f"Optimizing {model_name}...")
    
    #     storage_path = f"sqlite:///./optuna_studies/{model_name}.sqlite3"
    #     study = optuna.create_study(direction="minimize", storage=storage_path, study_name=model_name, load_if_exists=True)
    #     study.optimize(objective, n_trials=30)

    # ========================================================================

    # mlflow.set_experiment('SVR_Optuna_Hyperparameter_Optimization')

    # def objective_svr_(trial):
    #     params = {
    #         "C": trial.suggest_float("C", 0.1, 100, log=True),
    #         "epsilon": trial.suggest_float("epsilon", 0.01, 1.0, log=True),
    #         "gamma": trial.suggest_float('gamma', 1e-4, 1e-1, log=True),
    #         "kernel": trial.suggest_categorical("kernel", ["rbf", "poly", "linear"])
    #     }

    #     with mlflow.start_run(run_name=f"trial {trial.number}", nested=True) as run:
    #         # Regiser data for each trial
    #         dataset_source = "data\combined_file\combined.csv"  # your dataset path
    #         train_dataset = mlflow.data.from_pandas(train_df, source=dataset_source, name="Rice_Yield_Training_Data", targets="rice_yield")
    #         mlflow.log_input(train_dataset, context="training")


    #         trial.set_user_attr("mlflow_run_id", run.info.run_id) # Logging `run_id` on Optuna
    #         mlflow.set_tag("trial_number", trial.number) # Logging `trial number` on MLflow
    
    #         model = Pipeline([
    #         ('preprocessor', preprocessor),
    #         ('svr', SVR(**params))
    #         ])

    #         # rmse
    #         rmse_scores = cross_val_score(model, X_train, y_train, cv=tscv, scoring='neg_root_mean_squared_error')
    #         rmse = -np.mean(rmse_scores)

    #         # mae
    #         mae_scores = cross_val_score(model, X_train, y_train,cv=tscv, scoring='neg_mean_absolute_error' )
    #         mae = -np.mean(mae_scores)
        
    #         mlflow.log_metric('rmse', rmse)
    #         mlflow.log_metric('mae', mae)
    #         mlflow.log_params(params)
    #         mlflow.sklearn.log_model(model, name='model', input_example=X_train.iloc[:5])

    #     return rmse

    # ========================================================================

    # ==== Parent run ====
    # with mlflow.start_run(run_name="Optuna_Optimization_SVR") as parent_run:


    # # Example: log train data
    #     # Register Dataset
    #     dataset_source = "data\combined_file\combined.csv"  # your dataset path
    #     train_dataset = mlflow.data.from_pandas(train_df, source=dataset_source, name="Rice_Yield_Train_Data")
    #     # test_dataset = mlflow.data.from_pandas(X_test, source=dataset_source, name="Rice_Yield_Test_Data")

    #     mlflow.log_input(train_dataset, context="train")
    #     # mlflow.log_input(test_dataset, context="testing")

    #     # Create study
    #     study = optuna.create_study(study_name="SVR Trials", direction="minimize", storage="sqlite:///./optuna_svr_opt.sqlite3")

    #     # Optimize trials (logged as nested runs)
    #     study.optimize(objective_svr_, n_trials=10)

    #     # Best trial
    #     best_trial = study.best_trial
    #     best_run_id = best_trial.user_attrs["mlflow_run_id"]

    #     print("\nBest trial number:", best_trial.number)
    #     print("Best RMSE:", best_trial.value)
    #     print("Best params:", best_trial.params)
    #     print("Best trial MLflow run:", best_run_id)

    #     # ==== Retrain best model on full training data ====
    #     final_model = Pipeline(steps=[('preprocessor', preprocessor), ('svr', SVR(**best_params))])
    #     final_model.fit(X_train, y_train)

    #     preds = final_model.predict(X_test)
    #     final_rmse = np.sqrt(mean_squared_error(y_test, preds))

    #     # === Model signature ===
    #     # signature = infer_signature(X_train, final_model.predict(X_train))

    #     # ==== Log final model in top-level run ====
    #     mlflow.set_tag("best_trial_number", best_trial.number)
    #     mlflow.log_params(best_trial.params)
    #     mlflow.log_metric("final_rmse", final_rmse)
    #     mlflow.sklearn.log_model(final_model, 
    #                              name="best_final_model", 
    #                              input_example=X_train.iloc[:5],
    #                              registered_model_name='Best SVR Model')

    # ========================================================================

    # Common MLflow experiment
    # mlflow.set_experiment("Rice_Yield_Optuna_HPO")

    # # Dictionary of candidate models and their search spaces
    # model_spaces = {
    #     "SVR": lambda trial: {
    #         "C": trial.suggest_float("C", 0.1, 100, log=True),
    #         "epsilon": trial.suggest_float("epsilon", 0.01, 1.0, log=True),
    #         "gamma": trial.suggest_float('gamma', 1e-4, 1e-1, log=True),
    #         "kernel": trial.suggest_categorical("kernel", ["rbf", "poly", "linear"])
    #     },
    #     "AdaBoost": lambda trial: {
    #         "n_estimators": trial.suggest_int("n_estimators", 50, 200, step=50),
    #         "learning_rate": trial.suggest_float("learning_rate", 0.01, 1.0, log=True)
    #     }
    # }

    # # Mapping model name → actual sklearn class
    # model_classes = {
    #     "SVR": SVR,
    #     "AdaBoost": AdaBoostRegressor
    # }


    # # Reusable objective factory
    # def make_objective(model_name):
    #     def objective(trial):
    #         params = model_spaces[model_name](trial)

    #         with mlflow.start_run(run_name=f"{model_name}_trial_{trial.number}", nested=True) as run:
    #             # Register dataset (once per trial for traceability)
    #             dataset_source = "C:/Users/DELL/Desktop/crop_yield/data/combined_file/combined.csv"
    #             train_dataset = mlflow.data.from_pandas(train_df, source=dataset_source, 
    #                                                     name="Rice_Yield_Training_Data", 
    #                                                     targets="rice_yield")
    #             mlflow.log_input(train_dataset, context="training")

    #             trial.set_user_attr("mlflow_run_id", run.info.run_id)
    #             mlflow.set_tag("trial_number", trial.number)
    #             mlflow.set_tag("model_name", model_name)

    #             model = Pipeline([
    #                 ("preprocessor", preprocessor),
    #                 (model_name.lower(), model_classes[model_name](**params))
    #             ])

    #             # RMSE
    #             rmse_scores = cross_val_score(model, X_train, y_train, cv=tscv, scoring="neg_root_mean_squared_error")
    #             rmse = -np.mean(rmse_scores)

    #             # MAE
    #             mae_scores = cross_val_score(model, X_train, y_train, cv=tscv, scoring="neg_mean_absolute_error")
    #             mae = -np.mean(mae_scores)

    #             mlflow.log_metric("rmse", rmse)
    #             mlflow.log_metric("mae", mae)
    #             mlflow.log_params(params)
    #             mlflow.sklearn.log_model(model, name="model", input_example=X_train.iloc[:5])

    #         return rmse
    #     return objective


    # # ==== Parent run for ALL models ====
    # with mlflow.start_run(run_name="Optuna_HPO_All_Models") as parent_run:
    #     dataset_source = "data/combined_file/combined.csv"
    #     train_dataset = mlflow.data.from_pandas(train_df, source=dataset_source, name="Rice_Yield_Train_Data")
    #     mlflow.log_input(train_dataset, context="train")

    #     for model_name in model_spaces.keys():
    #         study = optuna.create_study(
    #             study_name=f"{model_name}_Trials",
    #             direction="minimize",
    #             storage="sqlite:///./optuna_all_models.sqlite3",  # Shared DB
    #             load_if_exists=True
    #         )
    #         study.optimize(make_objective(model_name), n_trials=10)

    #         best_trial = study.best_trial
    #         best_run_id = best_trial.user_attrs["mlflow_run_id"]

    #         print(f"\nModel: {model_name}")
    #         print("Best trial number:", best_trial.number)
    #         print("Best RMSE:", best_trial.value)
    #         print("Best params:", best_trial.params)
    #         print("Best trial MLflow run:", best_run_id)

    #         # Retrain best model
    #         final_model = Pipeline([
    #             ("preprocessor", preprocessor),
    #             (model_name.lower(), model_classes[model_name](**best_trial.params))
    #         ])
    #         final_model.fit(X_train, y_train)
    #         preds = final_model.predict(X_test)
    #         final_rmse = np.sqrt(mean_squared_error(y_test, preds))

    #         mlflow.set_tag(f"{model_name}_best_trial_number", best_trial.number)
    #         mlflow.log_params({f"{model_name}_{k}": v for k, v in best_trial.params.items()})
    #         mlflow.log_metric(f"{model_name}_final_rmse", final_rmse)
    #         mlflow.sklearn.log_model(final_model, 
    #                                  name=f"{model_name}_best_final_model", 
    #                                  input_example=X_train.iloc[:5],
    #                                  registered_model_name=f"Best_{model_name}_Model")

    # ========================================================================

    from sklearn.ensemble import RandomForestRegressor
    import joblib

    mlflow.set_experiment("Rice_Yield_Optuna_HPO")

    # === Candidate models and their search spaces ===
    search_spaces = {
        "SVR": {
            "model_class": SVR,
            "params": lambda trial: {
                "C": trial.suggest_float("C", 0.1, 100, log=True),
                "epsilon": trial.suggest_float("epsilon", 0.01, 1.0, log=True),
                "gamma": trial.suggest_float("gamma", 1e-4, 1e-1, log=True),
                "kernel": trial.suggest_categorical("kernel", ["rbf", "poly", "linear"])
            }
        },
        "RandomForest": {
            "model_class": RandomForestRegressor,
            "params": lambda trial: {
                "n_estimators": trial.suggest_int("n_estimators", 50, 200),
                "max_depth": trial.suggest_int("max_depth", 3, 15),
                "min_samples_split": trial.suggest_int("min_samples_split", 2, 10)
            }
        },
        "XGB": {
            "model_class": XGBRegressor,
            "params": lambda trial: {
                "n_estimators": trial.suggest_int("n_estimators", 50, 200),
                "max_depth": trial.suggest_int("max_depth", 3, 10),
                "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True)
            }
        }
    }

    def make_objective(model_name, model_class, param_fn):
        def objective(trial):
            params = param_fn(trial)

            with mlflow.start_run(run_name=f"{model_name}_trial_{trial.number}", nested=True) as run:
                trial.set_user_attr("mlflow_run_id", run.info.run_id)

                model = Pipeline([
                    ('preprocessor', preprocessor),
                    (model_name.lower(), model_class(**params))
                ])

                # cross-val metrics
                rmse_scores = cross_val_score(model, X_train, y_train, cv=tscv, scoring="neg_root_mean_squared_error")
                rmse = -np.mean(rmse_scores)
                mae_scores = cross_val_score(model, X_train, y_train, cv=tscv, scoring="neg_mean_absolute_error")
                mae = -np.mean(mae_scores)

                mlflow.log_params(params)
                mlflow.log_metric("rmse", rmse)
                mlflow.log_metric("mae", mae)

            return rmse
        return objective

    best_models = {}

    # === Loop over candidate models ===
    for model_name, cfg in search_spaces.items():
        with mlflow.start_run(run_name=f"{model_name}_Optimization") as parent_run:
            study = optuna.create_study(
                study_name=f"{model_name}_Study",
                direction="minimize",
                storage=f"sqlite:///./optuna_models.sqlite3"
            )

            study.optimize(make_objective(model_name, cfg["model_class"], cfg["params"]), n_trials=10)

            best_trial = study.best_trial
            best_run_id = best_trial.user_attrs["mlflow_run_id"]

            mlflow.set_tag("best_trial_number", best_trial.number)
            mlflow.log_params(best_trial.params)
            mlflow.log_metric("best_rmse", best_trial.value)

            # Retrain best model
            best_model = Pipeline([
                ('preprocessor', preprocessor),
                (model_name.lower(), cfg["model_class"](**best_trial.params))
            ])
            best_model.fit(X_train, y_train)

            preds = best_model.predict(X_test)
            final_rmse = np.sqrt(mean_squared_error(y_test, preds))

            mlflow.log_metric("final_rmse", final_rmse)
            mlflow.sklearn.log_model(
                best_model,
                name=f"{model_name}_final_model",
                input_example=X_train.iloc[:5],
                registered_model_name=f"{model_name}_Best_Model"
            )

            best_models[model_name] = {
                "rmse": final_rmse,
                "params": best_trial.params,
                "mlflow_run_id": parent_run.info.run_id
            }

            # === Inside your loop, after training best_model and evaluating ===
            # Save the retrained best model locally
            model_filename = f"{model_name}_best_model.pkl"
            joblib.dump(best_model, model_filename)
            print(f"Saved {model_name} best model as {model_filename}")


    print("Summary of best models:")
    print(best_models)

    # ========================================================================

    import mlflow
    import shap
    import matplotlib.pyplot as plt

    def run_shap_analysis(model_name, run_id, X_train, preprocessor):
        """
        Run SHAP analysis on a model stored in MLflow.
        """
        # Load the trained model from MLflow
        model_uri = f"runs:/{run_id}/{model_name}_final_model"
        model = mlflow.sklearn.load_model(model_uri)

        # Preprocess training data
        X_train_preprocessed = preprocessor.transform(X_train)
        estimator = model.named_steps[model_name.lower()]

        # Compute SHAP values
        explainer = shap.Explainer(estimator, X_train_preprocessed)
        shap_values = explainer(X_train_preprocessed)

        # Beeswarm plot
        plt.figure()
        shap.plots.beeswarm(shap_values, show=False)
        beeswarm_file = f"{model_name}_shap_beeswarm.png"
        plt.savefig(beeswarm_file)
        plt.close()

        # Summary bar plot
        plt.figure()
        shap.plots.bar(shap_values, show=False)
        summary_file = f"{model_name}_shap_summary.png"
        plt.savefig(summary_file)
        plt.close()

        print(f"SHAP analysis completed for {model_name}")
        return beeswarm_file, summary_file

    # ===== Example: Run SHAP on the two models you manually select =====
    selected_models = ["SVR", "XGB"]

    for model_name in selected_models:
        run_id = best_models[model_name]["mlflow_run_id"]
        run_shap_analysis(model_name, run_id, X_train, preprocessor)

    # ========================================================================



    return None


_static_assets = ["eda_notebook_files","eda_notebook_files\\figure-html\\cell-45-output-4.png","eda_notebook_files\\libs\\quarto-html\\tippy.css","eda_notebook_files\\libs\\quarto-html\\quarto-syntax-highlighting-d4d76bf8491c20bad77d141916dc28e1.css","eda_notebook_files\\libs\\bootstrap\\bootstrap-icons.css","eda_notebook_files\\libs\\bootstrap\\bootstrap-ddb3fb479c87e28f33ddcd4932113dd5.min.css","eda_notebook_files\\libs\\clipboard\\clipboard.min.js","eda_notebook_files\\libs\\quarto-html\\quarto.js","eda_notebook_files\\libs\\quarto-html\\popper.min.js","eda_notebook_files\\libs\\quarto-html\\tippy.umd.min.js","eda_notebook_files\\libs\\quarto-html\\anchor.min.js","eda_notebook_files\\libs\\bootstrap\\bootstrap.min.js"]
_static_assets = {"/" + sa: Path(__file__).parent / sa for sa in _static_assets}

app = App(
    Path(__file__).parent / "eda_notebook.html",
    server,
    static_assets=_static_assets,
)
